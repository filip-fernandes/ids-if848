{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mahjWWXCvCc4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from torch.nn.utils.parametrizations import spectral_norm\n",
        "from torch.nn.functional import cross_entropy, cosine_similarity\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "torch.manual_seed(9)\n",
        "np.random.seed(9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlNNurOtTveb"
      },
      "source": [
        "# Importar, preprocessar dividir os dados de treino. (KDD99 10%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sf0PTZ_aSZ7J"
      },
      "outputs": [],
      "source": [
        "!wget http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0are1VySgPI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import gzip\n",
        "\n",
        "dataset_path = 'kddcup.data_10_percent.gz'\n",
        "\n",
        "with gzip.open(dataset_path, 'rt') as f:\n",
        "    df = pd.read_csv(f, header=None)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xILkMjBb7iBi"
      },
      "outputs": [],
      "source": [
        "df = df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEchxqCb8Tja"
      },
      "outputs": [],
      "source": [
        "# Uma rápida análise nas labels dos dados\n",
        "col = df.iloc[:, 41]\n",
        "value_counts = col.value_counts().sort_values(ascending=False)\n",
        "print(f\"Value Counts for the label column\")\n",
        "print(value_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQWzFeazxV5b"
      },
      "outputs": [],
      "source": [
        "# Queremos apenas fazer predições nesses ataques.\n",
        "labels_allowed = ['normal.', 'neptune.', 'ipsweep.', 'portsweep.', 'pod.', 'satan.',  'guess_passwd.']\n",
        "labels = df.iloc[:, -1]\n",
        "mask = labels.isin(labels_allowed)\n",
        "df = df[mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8E4cXHBx517"
      },
      "outputs": [],
      "source": [
        "# Visualização do dataset apos remover as outras labels\n",
        "col = df.iloc[:, 41]\n",
        "value_counts = col.value_counts().sort_values(ascending=False)\n",
        "print(f\"Value Counts for the label column\")\n",
        "print(value_counts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Yte.unique(return_counts=True)[1]/ Yte.unique(return_counts=True)[1].sum()"
      ],
      "metadata": {
        "id": "4giYQ270iLP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ytr.unique(return_counts=True)[1].sum()"
      ],
      "metadata": {
        "id": "F7FLK7xZlPwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKMxp7HjUXsX"
      },
      "outputs": [],
      "source": [
        "# fazer one-hot nas variáveis categóricas (ex: protocolo = http, udp -> [0, 1], [1, 0])\n",
        "\n",
        "number_col = df.select_dtypes(include=['number']).columns # pegar colunas numéricas\n",
        "cat_col = df.columns.difference(number_col) # pegar colunas categóricas\n",
        "cat_col = cat_col.drop(41) # remover coluna do rótulo\n",
        "df_cat = df[cat_col].copy()\n",
        "one_hot_data = pd.get_dummies(df_cat, columns=cat_col) # one-hot\n",
        "one_hot_df = pd.concat([df, one_hot_data],axis=1)\n",
        "one_hot_df.drop(columns=cat_col, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45StFv_WUgmA"
      },
      "outputs": [],
      "source": [
        "dataset_x = torch.tensor(one_hot_df.drop(41, axis=1).values, dtype=torch.float32)\n",
        "dataset_y = torch.tensor(pd.factorize(one_hot_df[41])[0], dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKNWObPxVndC"
      },
      "outputs": [],
      "source": [
        "# split treino, teste e validação\n",
        "def split_data(dataset_x, dataset_y, tr_split, val_split, te_split):\n",
        "  assert tr_split + val_split + te_split == 1.0\n",
        "\n",
        "  # para reproducibilidade\n",
        "  torch.manual_seed(1)\n",
        "\n",
        "  permutation = torch.randperm(len(dataset_x))\n",
        "\n",
        "  dataset_size = len(dataset_x)\n",
        "  tr_size = int(dataset_size * tr_split)\n",
        "  val_size = int(dataset_size * val_split)\n",
        "  te_size = int(dataset_size * te_split)\n",
        "\n",
        "  train_idx = permutation[:tr_size]\n",
        "  val_idx = permutation[tr_size:tr_size + val_size]\n",
        "  test_idx = permutation[-te_size:]\n",
        "\n",
        "  Xtr = dataset_x[train_idx]\n",
        "  Ytr = dataset_y[train_idx]\n",
        "\n",
        "  Xval = dataset_x[val_idx]\n",
        "  Yval = dataset_y[val_idx]\n",
        "\n",
        "  Xte = dataset_x[test_idx]\n",
        "  Yte = dataset_y[test_idx]\n",
        "\n",
        "  return Xtr, Ytr, Xval, Yval, Xte, Yte\n",
        "\n",
        "tr_split = 0.7\n",
        "val_split = 0.15\n",
        "te_split = 1 - (tr_split + val_split)\n",
        "\n",
        "Xtr, Ytr, Xval, Yval, Xte, Yte = split_data(dataset_x, dataset_y, tr_split, val_split, te_split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpco42vRTD8h"
      },
      "outputs": [],
      "source": [
        "# definir normalizadores (um para cada feature)\n",
        "from sklearn import preprocessing\n",
        "\n",
        "minmax_scaler_vector = []\n",
        "\n",
        "for col in Xtr.transpose(0, 1):\n",
        "  minmax_scaler = preprocessing.MinMaxScaler()\n",
        "  minmax_scaler.fit(col.view(-1, 1))\n",
        "  minmax_scaler_vector.append(minmax_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1phDeMkIZmaU"
      },
      "outputs": [],
      "source": [
        "# normalizar os dados\n",
        "# sei que parece uma gambiarra, mas ficar nesse \"vai e vem\" de np.array e torch.tensor é mais rápido\n",
        "Xtr_normal = [minmax_scaler_vector[i].transform(col.view(-1, 1)) for i, col in enumerate(Xtr.transpose(0, 1))]\n",
        "Xval_normal = [minmax_scaler_vector[i].transform(col.view(-1, 1)) for i, col in enumerate(Xval.transpose(0, 1))]\n",
        "Xte_normal = [minmax_scaler_vector[i].transform(col.view(-1, 1)) for i, col in enumerate(Xte.transpose(0, 1))]\n",
        "\n",
        "Xtr_normal = torch.tensor(np.array(Xtr_normal, dtype=np.float32)).squeeze(-1).transpose(0, 1)\n",
        "Xval_normal = torch.tensor(np.array(Xval_normal, dtype=np.float32)).squeeze(-1).transpose(0, 1)\n",
        "Xte_normal = torch.tensor(np.array(Xte_normal, dtype=np.float32)).squeeze(-1).transpose(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paavZ1eolIBa"
      },
      "outputs": [],
      "source": [
        "# preparar os dados para a TMGGAN. Poderia utilizar um DataLoader, mas\n",
        "# por falta de tempo vou utilizar o mesmo método ineficiente utilizado\n",
        "# pelo código.\n",
        "unable_to_train = []\n",
        "unique_labels = torch.unique(dataset_y)\n",
        "XtrN_for_TMG = []\n",
        "for i, label in enumerate(unique_labels):\n",
        "    filtered_xs = Xtr_normal[Ytr == label].tolist()\n",
        "    XtrN_for_TMG.append(torch.tensor(filtered_xs))\n",
        "\n",
        "\n",
        "# XtrN_for_TMG -> cata sublista i contém a classe i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXzwhnf0XsOX"
      },
      "outputs": [],
      "source": [
        "# preparar DataLoaders para cada split\n",
        "tr_dataset = TensorDataset(Xtr_normal, Ytr)\n",
        "tr_dataloader = DataLoader(tr_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "val_dataset = TensorDataset(Xval_normal, Yval)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "te_dataset = TensorDataset(Xte_normal, Yte)\n",
        "te_dataloader = DataLoader(te_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ThUSterIavD"
      },
      "source": [
        "# Definir o modelo TMG-GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dt-53__y7BgQ"
      },
      "outputs": [],
      "source": [
        "def save_and_download(model, name='model'):\n",
        "  torch.save(model.state_dict(), f'{name}.pth')\n",
        "  from google.colab import files\n",
        "  files.download(f'{name}.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLvUKF9JkeZo"
      },
      "outputs": [],
      "source": [
        "def init_weights(layer: nn.Module):\n",
        "    if type(layer) == nn.Conv1d:\n",
        "        nn.init.normal_(layer.weight, 0.0, 0.02)\n",
        "        if layer.bias is not None:\n",
        "            nn.init.constant_(layer.bias, 0)\n",
        "    elif type(layer) == nn.BatchNorm1d:\n",
        "        nn.init.normal_(layer.weight, 1.0, 0.02)\n",
        "        nn.init.constant_(layer.bias, 0)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout1 = nn.Dropout(0.2)\n",
        "    self.dropout2 = nn.Dropout(0.2)\n",
        "    self.dropout3 = nn.Dropout(0.2)\n",
        "    self.dropout4 = nn.Dropout(0.2)\n",
        "    self.dropout5 = nn.Dropout(0.2)\n",
        "    self.conv1 = nn.ConvTranspose1d(100, 1024, 3, 1, 1)\n",
        "    self.conv2 = nn.ConvTranspose1d(1024, 512, 3, 1, 1)\n",
        "    self.conv3 = nn.ConvTranspose1d(512, 256, 3, 1, 1)\n",
        "    self.conv4 = nn.ConvTranspose1d(256, 128, 3, 1, 1)\n",
        "    self.conv5 = nn.ConvTranspose1d(128, 117, 3, 1, 1)\n",
        "    self.sigmoid = nn.ReLU()\n",
        "    self.hidden_status = None\n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.dropout1(self.conv1(x)))\n",
        "    x = self.relu(self.dropout2(self.conv2(x)))\n",
        "    x = self.relu(self.dropout3(self.conv3(x)))\n",
        "    x = self.relu(self.dropout4(self.conv4(x)))\n",
        "    x = self.relu(self.dropout5(self.conv5(x)))\n",
        "    self.hidden_status = x\n",
        "    return self.sigmoid(x)\n",
        "\n",
        "  def generate_samples(self, num: int) -> torch.Tensor:\n",
        "    z = torch.randn(num, 100, 1, device=device)\n",
        "    return self.forward(z).squeeze(-1).unsqueeze(1)\n",
        "\n",
        "\n",
        "class DiscriminatorClassifier(nn.Module):\n",
        "    def __init__(self, ndf, nc):\n",
        "        super(DiscriminatorClassifier, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            spectral_norm(nn.Conv1d(nc, ndf, 9, 3, 0, bias=False)),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            spectral_norm(nn.Conv1d(ndf, ndf*2, 6, 3, 0, bias=False)),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            spectral_norm(nn.Conv1d(ndf*2, ndf*2, 6, 1, 0, bias=False)),\n",
        "            nn.Dropout(0.2),\n",
        "        )\n",
        "        self.leaky = nn.LeakyReLU(0.2, inplace=False)\n",
        "        self.dfc =  nn.Sequential(\n",
        "            spectral_norm(nn.Conv1d(ndf*2, ndf, 3, 1, 0, bias=False))\n",
        "        )\n",
        "        self.cfc = nn.Sequential(\n",
        "            spectral_norm(nn.Conv1d(ndf*2, ndf, 3, 1, 0, bias=False))\n",
        "        )\n",
        "        self.l1 = spectral_norm(nn.Linear(768, 1))\n",
        "        self.l2 = spectral_norm(nn.Linear(768, 7))\n",
        "        self.apply(init_weights)\n",
        "        self.hidden_status = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.main(input)\n",
        "        self.hidden_status = x\n",
        "        x1 = self.leaky(self.dfc(x))\n",
        "        x2 = self.leaky(self.cfc(x).squeeze(-1))\n",
        "        x1 = x.view(x.shape[0], -1)\n",
        "        x2 = x.view(x.shape[0], -1)\n",
        "        return self.l1(x1), self.l2(x2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TMG ORIGINAL - SEM DCGAN\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "    self.main = nn.Sequential(\n",
        "        nn.Linear(128, 512),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.LeakyReLU(inplace=True),\n",
        "        nn.Linear(512, 128),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.LeakyReLU(inplace=True),\n",
        "        nn.Linear(128, 32),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.LeakyReLU(inplace=True),\n",
        "    )\n",
        "    self.hidden_status = None\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(32, 117),\n",
        "        nn.Sigmoid(),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.main(x)\n",
        "    self.hidden_status = x\n",
        "    return self.fc(x)\n",
        "\n",
        "  def generate_samples(self, num: int) -> torch.Tensor:\n",
        "    z = torch.randn(num, 128, device=device)\n",
        "    return self.forward(z)\n",
        "\n",
        "\n",
        "class DiscriminatorClassifier(nn.Module):\n",
        "    def __init__(self, ndf, nc):\n",
        "        super(DiscriminatorClassifier, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            spectral_norm(nn.Conv1d(nc, ndf, 9, 3, 0, bias=False)),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            spectral_norm(nn.Conv1d(ndf, ndf*2, 6, 3, 0, bias=False)),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            spectral_norm(nn.Conv1d(ndf*2, ndf*2, 6, 2, 0, bias=False)),\n",
        "            nn.Dropout(0.2),\n",
        "        )\n",
        "        self.leaky = nn.LeakyReLU(0.2, inplace=False)\n",
        "        self.dfc =  nn.Sequential(\n",
        "            (nn.Conv1d(ndf*2, 1, 3, 1, 0, bias=False))\n",
        "        )\n",
        "        self.cfc = nn.Sequential(\n",
        "            spectral_norm(nn.Conv1d(ndf*2, 7, 3, 1, 0, bias=False))\n",
        "        )\n",
        "        self.apply(init_weights)\n",
        "        self.hidden_status = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.main(input)\n",
        "        self.hidden_status = x\n",
        "        x1 = self.leaky(self.dfc(x))\n",
        "        x2 = self.leaky(self.cfc(x).squeeze(-1))\n",
        "        return x1, x2"
      ],
      "metadata": {
        "id": "kB0POJE16Vcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rX-ZWlVluyT5"
      },
      "outputs": [],
      "source": [
        "def gradient_penalty(critic, real, fake, lambda_gp):\n",
        "    # calcular o gradient penalty\n",
        "    batch_size = real.size(0)\n",
        "    epsilon = torch.rand(batch_size, 1, 1).to(real.device)\n",
        "    interpolated = interpolate(real, fake, epsilon)\n",
        "    critic_scores_interpolated = critic(interpolated)[0]\n",
        "    gradients_interpolated = torch.autograd.grad(\n",
        "        outputs=critic_scores_interpolated, inputs=interpolated,\n",
        "        grad_outputs=torch.ones_like(critic_scores_interpolated),\n",
        "        create_graph=True, retain_graph=True\n",
        "    )[0]\n",
        "    # norma L2\n",
        "    gradients_norm = gradients_interpolated.norm(2, dim=1)\n",
        "    gradient_penalty = lambda_gp * ((gradients_norm - 1) ** 2).mean()\n",
        "    return gradient_penalty\n",
        "\n",
        "def interpolate(real, fake, epsilon):\n",
        "  return epsilon * real + (1 - epsilon) * fake"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TMGGAN(nn.Module):\n",
        "    def __init__(self, hyperparams, dataset_info):\n",
        "        super().__init__()\n",
        "        self.generators = nn.ModuleList([Generator().to(device) for i in range(dataset_info[\"num_labels\"])])\n",
        "        self.cd = DiscriminatorClassifier(hyperparams['ndf'], hyperparams['nc']).to(device)\n",
        "\n",
        "        self.batch_size = hyperparams['batch_size']\n",
        "        self.epochs     = hyperparams['epochs']\n",
        "        self.c_loop_num = hyperparams['cd_loop_num']\n",
        "        self.lr         = hyperparams['lr']\n",
        "        self.clip_value = hyperparams['clip_value']\n",
        "\n",
        "        self.labels       = dataset_info['labels']\n",
        "        self.num_labels   = dataset_info['num_labels']\n",
        "        self.num_features = dataset_info['num_features']\n",
        "        self.samples      = dataset_info['samples']\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.gLoss = []\n",
        "        self.cLoss = []\n",
        "\n",
        "    def fit(self):\n",
        "        # colocar geradores e discriminador em modo de treino\n",
        "        for generator in self.generators:\n",
        "            generator.train()\n",
        "        self.cd.train()\n",
        "        # inicializar otimizadores\n",
        "        cd_optimizer = torch.optim.Adam(\n",
        "            params=self.cd.parameters(),\n",
        "            lr=self.lr,\n",
        "            betas=(0.9, 0.999)\n",
        "        )\n",
        "        g_optimizers = [\n",
        "            torch.optim.Adam(\n",
        "                params=self.generators[i].parameters(),\n",
        "                lr=0.0002,\n",
        "                betas=(0.9, 0.999),\n",
        "            )\n",
        "            for i in range(self.num_labels)\n",
        "        ]\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        # loop de treino\n",
        "        for epoch in range(self.epochs):\n",
        "            print(epoch/self.epochs)\n",
        "            for label in self.labels:\n",
        "                # treinar discriminador e classificador\n",
        "                for _ in range(5):\n",
        "                    cd_optimizer.zero_grad()\n",
        "                    real_samples = self._get_target_samples(label, self.batch_size).to(device)\n",
        "                    score_real, predicted_labels = self.cd(real_samples)\n",
        "                    score_real = score_real.mean()\n",
        "                    generated_samples = self.generators[label].generate_samples(self.batch_size).to(device)\n",
        "                    score_generated = self.cd(generated_samples)[0].mean()\n",
        "                    d_loss = (score_generated - score_real) / 2\n",
        "                    c_loss = criterion(\n",
        "                        predicted_labels.squeeze(1),\n",
        "                        torch.full([len(predicted_labels)], label, device=device),\n",
        "                    )\n",
        "                    self.cLoss.append(c_loss)\n",
        "                    #gp = gradient_penalty(self.cd, real_samples, generated_samples, 10)\n",
        "                    loss = d_loss + c_loss\n",
        "                    loss.backward()\n",
        "                    cd_optimizer.step()\n",
        "\n",
        "                # treinar gerador\n",
        "                g_optimizers[label].zero_grad()\n",
        "                generated_samples = self.generators[label].generate_samples(self.batch_size).to(device)\n",
        "                real_samples = self._get_target_samples(label, self.batch_size).to(device)\n",
        "\n",
        "                self.cd(real_samples)\n",
        "                hidden_real = self.cd.hidden_status\n",
        "\n",
        "                score_generated, predicted_labels = self.cd(generated_samples)\n",
        "                hidden_generated = self.cd.hidden_status\n",
        "\n",
        "                cd_hidden_loss = -cosine_similarity(hidden_real, hidden_generated).mean()\n",
        "\n",
        "                score_generated = score_generated.mean()\n",
        "                loss_label = criterion(\n",
        "                    predicted_labels.squeeze(1),\n",
        "                    torch.full([len(predicted_labels)], label, device=device),\n",
        "                )\n",
        "\n",
        "                if epoch <= self.epochs/2:\n",
        "                    cd_hidden_loss = 0\n",
        "\n",
        "                g_loss = -score_generated + loss_label + cd_hidden_loss\n",
        "                g_loss.backward()\n",
        "                g_optimizers[label].step()\n",
        "                self.gLoss.append(g_loss.item())\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                torchvision.utils.save_image(generated_samples.squeeze(1), f\"outpute_{epoch}.png\", normalize=True)\n",
        "\n",
        "            #treinar os geradores com a similaridade cosseno\n",
        "            for optim in g_optimizers:\n",
        "                optim.zero_grad()\n",
        "            for i in self.generators:\n",
        "                i.generate_samples(3)\n",
        "            g_hidden_losses = []\n",
        "            for i, _ in enumerate(self.generators):\n",
        "                for j, _ in enumerate(self.generators):\n",
        "                    if i != j:\n",
        "                        g_hidden_losses.append(\n",
        "                            cosine_similarity(\n",
        "                                self.generators[i].hidden_status,\n",
        "                                self.generators[j].hidden_status,\n",
        "                            )\n",
        "                        )\n",
        "            g_hidden_loss = torch.mean(torch.stack(g_hidden_losses)) / self.num_features\n",
        "            g_hidden_loss.backward()\n",
        "            for i in g_optimizers:\n",
        "                i.step()\n",
        "\n",
        "        # treino finalizado, preparar para teste/validação\n",
        "        self.cd.eval()\n",
        "        for i in self.generators:\n",
        "            i.eval()\n",
        "\n",
        "    def _get_target_samples(self, label, num):\n",
        "        samples = torch.stack(random.choices(self.samples[label],k=num))\n",
        "        return samples.unsqueeze(1)\n",
        "\n",
        "    def generate_samples(self, target_label: int, num: int):\n",
        "        return self.generators[target_label].generate_samples(num).cpu().detach()\n",
        "\n",
        "    def generate_qualified_samples(self, target_label, num):\n",
        "        result = []\n",
        "        patience = 10\n",
        "        while len(result) < num:\n",
        "            sample = self.generators[target_label].generate_samples(1)\n",
        "            label = torch.argmax(self.cd(sample)[1])\n",
        "            if label == target_label or patience == 0:\n",
        "                result.append(sample.cpu().detach())\n",
        "                patience = 10\n",
        "            else:\n",
        "                patience -= 1\n",
        "        return torch.cat(result)\n"
      ],
      "metadata": {
        "id": "hx9zEMu3NySG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhHoozYK7TLB"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# ------ hyperparams -------- #\n",
        "hyperparams = {\n",
        "    'batch_size': 64,\n",
        "    'epochs': 1000,\n",
        "    'cd_loop_num': 5,\n",
        "    'lr': 2e-4,\n",
        "    'clip_value': 0.01,\n",
        "    'ndf': 64,\n",
        "    'nc':  1,\n",
        "}\n",
        "dataset_info = {\n",
        "    'labels': [i for i in range(len(XtrN_for_TMG))],\n",
        "    'samples': XtrN_for_TMG,\n",
        "    'num_labels': len(XtrN_for_TMG),\n",
        "    'num_features': len(Xtr_normal[0]),\n",
        "}\n",
        "# ---------------------------  #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sKLh0hwIozn"
      },
      "source": [
        "# Treinar, validar e testar o modelo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsY0_muN034M"
      },
      "outputs": [],
      "source": [
        "tmg = TMGGAN(hyperparams, dataset_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tmg.fit()"
      ],
      "metadata": {
        "id": "aG2hb_E5oiFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmg.load_state_dict(torch.load('funciona.pth'))"
      ],
      "metadata": {
        "id": "INkKwa_pKzdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyglfKZCv2xF"
      },
      "outputs": [],
      "source": [
        "save_and_download(tmg, 'tmg_original')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7lNHIx745Vk"
      },
      "outputs": [],
      "source": [
        "# testar classificador, obtendo precision, recall e f1\n",
        "def test_val(model, loader):\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    positive = 0\n",
        "    negative = 0\n",
        "    matrix = torch.zeros((7,7), dtype=int).to(device)\n",
        "    binary_matrix = torch.zeros((2,2), dtype=int).to(device)\n",
        "    y_true = torch.tensor([]).to(device)\n",
        "    y_scores = torch.tensor([]).to(device)\n",
        "    with torch.no_grad():\n",
        "        iter = 0\n",
        "        loss_sum = 0\n",
        "        for i, data in enumerate(loader):\n",
        "            X, y = data\n",
        "            X = X.unsqueeze(1).to(device)\n",
        "            y = y.int().to(device)\n",
        "            y_pred = model(X)[1].squeeze(0)\n",
        "            loss = criterion(y_pred.squeeze(1), y.long())\n",
        "            loss_sum += loss.item()\n",
        "            iter += 1\n",
        "            y_hat= torch.argmax(torch.nn.functional.softmax(y_pred.squeeze(1), dim=1), dim=1);\n",
        "\n",
        "            # matriz de confusão para classificação\n",
        "            for i in range(len(y)):\n",
        "                matrix[y[i], y_hat[i]] += 1\n",
        "            positive += torch.sum(y_hat == y)\n",
        "            negative += torch.sum(y_hat != y)\n",
        "\n",
        "            # matriz de confusão para classificação binária\n",
        "            correct = torch.eq(y_hat, y)\n",
        "            incorrect = torch.logical_not(correct)\n",
        "\n",
        "            binary_matrix[0, 0] += torch.sum(correct[y == 0])\n",
        "            binary_matrix[0, 1] += torch.sum(incorrect[y == 0])\n",
        "\n",
        "            binary_matrix[1, 1] += torch.sum(correct[y != 0])\n",
        "            binary_matrix[1, 0] += torch.sum(incorrect[y != 0])\n",
        "\n",
        "            y_true = torch.cat((y_true, y))\n",
        "            y_scores = torch.cat((y_scores, y_hat))\n",
        "\n",
        "    acc = positive / (positive + negative)\n",
        "    avg_loss = loss_sum / iter\n",
        "    print(positive)\n",
        "    print(negative)\n",
        "    print(\"Accuracy:\", acc.item())\n",
        "    print(\"Average Loss:\", avg_loss)\n",
        "\n",
        "    y_true[y_true != 0] = 1\n",
        "    y_scores[y_scores != 0] = 1\n",
        "\n",
        "    return matrix, binary_matrix, y_true, y_scores\n",
        "\n",
        "def train(model, epochs, optimizer, criterion, loader):\n",
        "  lossi = []\n",
        "  for epoch in range(epochs):\n",
        "    for i, data in enumerate(loader):\n",
        "        X, y = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X.unsqueeze(1).to(device))\n",
        "        loss = criterion(outputs[1].squeeze(1), y.to(torch.int64).to(device))\n",
        "        loss.backward();\n",
        "        optimizer.step();\n",
        "        lossi.append(loss.item())\n",
        "  return lossi\n",
        "\n",
        "def aucroc(y_true, y_scores):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgT4oJDnMJiP"
      },
      "outputs": [],
      "source": [
        "def plot_confusion(matrix):\n",
        "    fig, ax = plt.subplots()\n",
        "    matrix = matrix.cpu()\n",
        "    matrix = matrix/(matrix.sum(axis=0) + 0.00001)\n",
        "    cax = ax.matshow(matrix, cmap='coolwarm')\n",
        "    plt.colorbar(cax)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCpAmnSdBSi-"
      },
      "outputs": [],
      "source": [
        "def print_metrics(matrix):\n",
        "  tp = matrix[0][0]\n",
        "  tn = matrix[1][1]\n",
        "  fp = matrix[1][0]\n",
        "  fn = matrix[0][1]\n",
        "  precision = tp/(tp + fp)\n",
        "  recall = tp/(tp + fn)\n",
        "  print(\"Precision:\", precision.item())\n",
        "  print(\"Recall:\", recall.item())\n",
        "  print(\"F1:\", (2 * precision * recall) / (precision + recall).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MykNox1xNYd3"
      },
      "outputs": [],
      "source": [
        "def enhance_train_data():\n",
        "  Ytr_enhanced = torch.tensor([])\n",
        "  Xtr_enhanced = torch.tensor([])\n",
        "  for i, class_ in enumerate(XtrN_for_TMG):\n",
        "    with torch.no_grad():\n",
        "      new_samples = tmg.generate_qualified_samples(i, 1000).squeeze(1)\n",
        "      cat = torch.cat((class_, new_samples))\n",
        "      Xtr_enhanced = torch.cat((Xtr_enhanced, cat))\n",
        "      new_Y = torch.full([len(cat)], i)\n",
        "      Ytr_enhanced = torch.cat((Ytr_enhanced, new_Y))\n",
        "      print(i/7)\n",
        "  dataset = TensorDataset(Xtr_enhanced, Ytr_enhanced)\n",
        "  dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "  return dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVk8Se_sSaNb"
      },
      "outputs": [],
      "source": [
        "def fit_model(model, loader, epochs):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  lr = 1e-4\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  train(model, epochs, optimizer, criterion, loader)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGtJJfO4EETD"
      },
      "outputs": [],
      "source": [
        "model = fit_model(DiscriminatorClassifier(64, 1).to(device), tr_dataloader, epochs=1)\n",
        "matrix, binary, ytrue, ypred = test_val(model, val_dataloader)\n",
        "print_metrics(binary)\n",
        "plot_confusion(matrix)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vamos utilizar a TMG GAN para gerar dados sintéticos\n",
        "dataloader = enhance_train_data()"
      ],
      "metadata": {
        "id": "R8uVPd33llZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNDM7dXXEsIu"
      },
      "outputs": [],
      "source": [
        "model_enhanced_tmg = fit_model(DiscriminatorClassifier(64, 1).to(device), dataloader, 1)\n",
        "matrix_tmg, binary_tmg, ytrue, ypred = test_val(model_enhanced_tmg, te_dataloader)\n",
        "print_metrics(binary_tmg)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion(matrix_tmg)"
      ],
      "metadata": {
        "id": "qsGCPIoXmh5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_tmg"
      ],
      "metadata": {
        "id": "zq33_iXUGuWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trabalhos relacionados: outros métodos de data enhancing\n",
        "\n",
        "### SMOTE E SNGAN"
      ],
      "metadata": {
        "id": "UkgJpQjDhcxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMOTE"
      ],
      "metadata": {
        "id": "QWvnJUZHhovv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imblearn\n",
        "import imblearn.over_sampling as o_s\n"
      ],
      "metadata": {
        "id": "GPluGtyWkfi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smote = o_s.SMOTE(random_state=42)\n",
        "X_res, y_res = smote.fit_resample(Xtr_normal,Ytr)"
      ],
      "metadata": {
        "id": "nuu1fafSbHmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adasyn_data = DataLoader(TensorDataset(torch.tensor(X_res), torch.tensor(y_res)), batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "KtC4e1SJbLza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ada(model, epochs, batch_size, optimizer, criterion, loader):\n",
        "  lossi = []\n",
        "  for epoch in range(epochs):\n",
        "    for iter, data in enumerate(loader):\n",
        "        X, y = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X.unsqueeze(1).to(device))\n",
        "        loss = criterion(outputs[1].squeeze(1), y.to(torch.int64).to(device))\n",
        "        loss.backward();\n",
        "        lossi.append(loss.item());\n",
        "        optimizer.step();\n",
        "\n",
        "  print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
        "  return lossi"
      ],
      "metadata": {
        "id": "aE8005wdbX3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_smote = fit_model(DiscriminatorClassifier(64, 1).to(device), adasyn_data, 1)\n",
        "matrix_smote, binary_smote, ytrue_smote, ypred_smote= test_val(model_smote, loader=te_dataloader)\n",
        "print_metrics(binary_smote)"
      ],
      "metadata": {
        "id": "sdaEOwl-baaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion(matrix_smote)"
      ],
      "metadata": {
        "id": "grz67qk6fy9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SNGAN\n",
        "Utilizaremos o mesmo Critic e Gerador da TMG-GAN"
      ],
      "metadata": {
        "id": "qeYcstqLnpkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(layer: nn.Module):\n",
        "    if type(layer) == nn.Conv1d:\n",
        "        nn.init.normal_(layer.weight, 0.0, 0.02)\n",
        "        if layer.bias is not None:\n",
        "            nn.init.constant_(layer.bias, 0)\n",
        "    elif type(layer) == nn.BatchNorm1d:\n",
        "        nn.init.normal_(layer.weight, 1.0, 0.02)\n",
        "        nn.init.constant_(layer.bias, 0)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout1 = nn.Dropout(0.2)\n",
        "    self.dropout2 = nn.Dropout(0.2)\n",
        "    self.dropout3 = nn.Dropout(0.2)\n",
        "    self.dropout4 = nn.Dropout(0.2)\n",
        "    self.conv1 = nn.ConvTranspose1d(256, 128, 3, 1, 1)\n",
        "    self.conv2 = nn.ConvTranspose1d(128, 256, 3, 1, 1)\n",
        "    self.conv3 = nn.ConvTranspose1d(256, 512, 3, 1, 1)\n",
        "    self.conv4 = nn.ConvTranspose1d(512, 117, 3, 1, 1)\n",
        "    self.sigmoid = nn.ReLU()\n",
        "    self.hidden_status = None\n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.dropout1(self.conv1(x)))\n",
        "    x = self.relu(self.dropout2(self.conv2(x)))\n",
        "    x = self.relu(self.dropout3(self.conv3(x)))\n",
        "    x = self.relu(self.dropout4(self.conv4(x)))\n",
        "    self.hidden_status = x\n",
        "    return self.sigmoid(x)\n",
        "\n",
        "  def generate_samples(self, num: int) -> torch.Tensor:\n",
        "    z = torch.randn(num, 256, 1, device=device)\n",
        "    return self.forward(z).squeeze(-1).unsqueeze(1)\n",
        "\n",
        "\n",
        "class DiscriminatorClassifier(nn.Module):\n",
        "    def __init__(self, ndf, nc):\n",
        "        super(DiscriminatorClassifier, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            spectral_norm(nn.Conv1d(nc, ndf, 9, 3, 0, bias=False)),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            spectral_norm(nn.Conv1d(ndf, ndf*2, 6, 3, 0, bias=False)),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            spectral_norm(nn.Conv1d(ndf*2, ndf*2, 6, 1, 0, bias=False)),\n",
        "            nn.Dropout(0.2),\n",
        "        )\n",
        "        self.leaky = nn.LeakyReLU(0.2, inplace=False)\n",
        "        self.dfc =  nn.Sequential(\n",
        "            spectral_norm(nn.Conv1d(ndf*2, ndf, 3, 1, 0, bias=False))\n",
        "        )\n",
        "        self.cfc = nn.Sequential(\n",
        "            spectral_norm(nn.Conv1d(ndf*2, ndf, 3, 1, 0, bias=False))\n",
        "        )\n",
        "        self.l1 = spectral_norm(nn.Linear(768, 1))\n",
        "        self.l2 = spectral_norm(nn.Linear(768, 8))\n",
        "        self.apply(init_weights)\n",
        "        self.hidden_status = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.main(input)\n",
        "        self.hidden_status = x\n",
        "        x1 = self.leaky(self.dfc(x))\n",
        "        x2 = self.leaky(self.cfc(x).squeeze(-1))\n",
        "        x1 = x.view(x.shape[0], -1)\n",
        "        x2 = x.view(x.shape[0], -1)\n",
        "        return self.l1(x1), self.l2(x2)"
      ],
      "metadata": {
        "id": "18_kDhySn0Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = Generator().to(device)\n",
        "d = DiscriminatorClassifier(1, 64).to(device)"
      ],
      "metadata": {
        "id": "H2ZjCu32uZMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# treinar discriminador e classificador\n",
        "opt_d = torch.optim.Adam( d.parameters(), lr=2e-4,)\n",
        "opt_g = torch.optim.Adam( g.parameters(), lr=2e-4,)\n",
        "for i, data in enumerate(tr_dataloader):\n",
        "  X, y = data\n",
        "  X = X.to(device)\n",
        "  y = y.to(device)\n",
        "  for _ in range(5):\n",
        "      opt_d.zero_grad()\n",
        "      score_real = d(X.unsqueeze(-1))[0].mean()\n",
        "      generated_samples = g.generate_samples(64)\n",
        "      score_generated = d(generated_samples)[0].mean()\n",
        "      d_loss = (score_generated - score_real) / 2\n",
        "      d_loss.backward()\n",
        "      opt_d.step()\n",
        "  # treinar gerador\n",
        "  opt_g.zero_grad()\n",
        "  generated_samples = g.generate_samples(64)\n",
        "  score_generated = d(generated_samples)[0].mean()\n",
        "  g_loss = -score_generated\n",
        "  g_loss.backward()\n",
        "  g_opt.step()\n"
      ],
      "metadata": {
        "id": "85BRFxafn_gJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git init"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eq63L03V2xNY",
        "outputId": "9b09e81e-03b1-4cbc-b675-f94119177067"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/.git/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn91eoS-2yFZ",
        "outputId": "e7a7aff4-71ac-46c8-c8b2-6c52ad6679b7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: remote origin already exists.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nlNNurOtTveb"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
